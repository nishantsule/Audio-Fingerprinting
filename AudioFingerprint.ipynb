{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from scipy.signal import spectrogram\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.measure import compare_ssim\n",
    "from datasketch import MinHash\n",
    "import pyaudio\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure \n",
    "from operator import itemgetter\n",
    "import warnings\n",
    "import sys\n",
    "import pickle \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for tuning the Audiofingerprinting algorithm\n",
    "\n",
    "# Parameters used in generating spectrogram\n",
    "#----------------------------------\n",
    "nperseg = 16 * 256  # window size\n",
    "overlap_ratio = 0.4  # degree of overlap, larger number->more overlap, denser fingerprint\n",
    "#----------------------------------\n",
    "\n",
    "# Parameters used in finding local peaks\n",
    "#-------------------------\n",
    "min_peak_sep = 20  # larger sep -> less peaks -> less accuracy, but faster fingerprinting\n",
    "min_peak_amp = 10  # larger min amp -> less peaks -> less accuracy, but faster fingerprinting\n",
    "#-------------------------\n",
    "\n",
    "# Parameters used in generating fingerprint\n",
    "#------------------------------\n",
    "peak_connectivity = 15  # Number of neighboring peaks to use as target for each anchor\n",
    "peak_time_delta_min = 0  # Minimum spacing in time between peaks for anchor and target\n",
    "peak_time_delta_max = 200  # Maximum spacing in time between peaks for anchor and target\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class defines an AudioFP object that stores the name a song and its fingerprint. \n",
    "# It also contains all the functions used to read/record audio, generate spectrogram, find peaks,\n",
    "# generate fingerprint, and saving the object to file.\n",
    "\n",
    "class AudioFP():\n",
    "    \n",
    "    # Initializing AudioFP object.\n",
    "    \n",
    "    # Creating the AudioFP class object will prompt the user to chose whether they would like to read audio\n",
    "    # from a file or to record using a microphone, or to open an already saved object. \n",
    "    # Enter 'f' to read an audio file, 'r' to record, or 's' to open a saved object. \n",
    "    # Entering any other character will cause the program to throw an exception and exit.\n",
    "    # The user is also prompted to choose whether they want to generate plots. \n",
    "    # Enter 'y' to generate plots or 'n' to skip plotting.\n",
    "    # After these user selections are made, the program automatically reads/records audio, generate\n",
    "    # a sprectrogram, finds the local peaks in the spectrogram, and generates a fingerprint \n",
    "    # or simply reads an existing AudioFP object from file.\n",
    "    # Finally, if the user chose to read audio from file or record it, they are prompted to choose\n",
    "    # whether they want to save the object to file. Enter 'y' to save or 'n' to skip.\n",
    "\n",
    "    def __init__(self):\n",
    "        audio_type = input('Enter \"f\" to read from file, \"r\" to record, or \"s\" to open saved object: ')\n",
    "        if audio_type == 'f':\n",
    "            filename = input('Enter the filename you want to read excluding the extension: ')\n",
    "            self.songname = filename\n",
    "            if input('Do you want to show all plots? Enter \"y\" or \"n\": ') == 'y':\n",
    "                plot = True\n",
    "            else:\n",
    "                plot = False\n",
    "            channels, framerate = self.read_audiofile(plot)\n",
    "            f, t, sgram = self.generate_spectrogram(plot, channels, framerate)\n",
    "            fp, tp, peaks = self.find_peaks(plot, f, t, sgram)\n",
    "            self.generate_fingerprint(plot, fp, tp, peaks)\n",
    "            if input('Do you want to save the object to file for later use? Enter \"y\" or \"n\": ') == 'y':\n",
    "                print('Saving the object')\n",
    "                self.save_fingerprint()\n",
    "            else:\n",
    "                print('Not saving the object')\n",
    "        elif audio_type == 'r':\n",
    "            filename = input('Enter a name for the recording:')\n",
    "            self.songname = filename  \n",
    "            if input('Do you want to show all plots? Enter \"y\" or \"n\": ') == 'y':\n",
    "                plot = True\n",
    "            else:\n",
    "                plot = False\n",
    "            channels, framerate = self.record_audiofile(plot)\n",
    "            f, t, sgram = self.generate_spectrogram(plot, channels, framerate)\n",
    "            fp, tp, peaks = self.find_peaks(plot, f, t, sgram)\n",
    "            self.generate_fingerprint(plot, fp, tp, peaks)\n",
    "            if input('Do you want to save the object to file for later use? Enter \"y\" or \"n\": ') == 'y':\n",
    "                print('Saving the object')\n",
    "                self.save_fingerprint()\n",
    "            else:\n",
    "                print('Not saving the object')\n",
    "        elif audio_type == 's':\n",
    "            objname = input('Enter the filename where the object is saved: ')\n",
    "            objname = objname + '.pkl'\n",
    "            with open(objname, 'rb') as inputobj:\n",
    "                data = pickle.load(inputobj)\n",
    "                self.songname = data.songname\n",
    "                self.fingerprint = data.fingerprint\n",
    "            if input('Do you want to show all plots? Enter \"y\" or \"n\": ') == 'y':\n",
    "                plot = True\n",
    "                print('Songname: ', self.songname)\n",
    "                print('Audio-fingerprint: ', self.fingerprint)\n",
    "            else:\n",
    "                plot = False\n",
    "        else:\n",
    "            sys.exit('''Error: Incorrect entry. Enter \"f\" to read an audio file, \n",
    "                     \"r\" to record, or \"s\" to open a saved object''')\n",
    "        \n",
    "    # Read audio file using pydub and plot signal.\n",
    "    # The audio file has to be .mp3 format\n",
    "    def read_audiofile(self, plot):\n",
    "        songdata = []  # Empty list for holding audio data\n",
    "        channels = []  # Empty list to hold data from separate channels\n",
    "        audiofile = AudioSegment.from_file(self.songname + '.mp3')\n",
    "        songdata = np.frombuffer(audiofile._data, np.int16)\n",
    "        for chn in range(audiofile.channels):\n",
    "            channels.append(songdata[chn::audiofile.channels])  # separate signal from channels\n",
    "        framerate = audiofile.frame_rate\n",
    "        # Plot time signal\n",
    "        if plot:\n",
    "            p1 = figure(plot_width=900, plot_height=500, title='Audio Signal', \n",
    "                        x_axis_label='Time (s)', y_axis_label='Amplitude (arb. units)')\n",
    "            time = np.linspace(0, len(channels[0])/framerate, len(channels[0]))\n",
    "            p1.line(time[0::1000], channels[0][0::1000])\n",
    "            show(p1)\n",
    "        return channels, framerate\n",
    "            \n",
    "    # Record audio file using pyaudio and plot signal\n",
    "    def record_audiofile(self, plot):\n",
    "        rec_time = int(input('How long do you want to record? Enter time in seconds: '))\n",
    "        start_rec = input('Do you want to start recoding? Enter \"y\" to start:')\n",
    "        if start_rec=='y':\n",
    "            chk_size = 8192  # chunk size\n",
    "            fmt = pyaudio.paInt16  # format of audio \n",
    "            chan = 2  # Number of channels \n",
    "            samp_rate = 44100  # sampling rate\n",
    "            framerate = samp_rate\n",
    "            p = pyaudio.PyAudio()  # Initializing pyaudio object to open audio stream\n",
    "            astream = p.open(format=fmt, channels=chan, rate=samp_rate,\n",
    "                             input=True, frames_per_buffer=chk_size)\n",
    "            songdata = []\n",
    "            channels = []\n",
    "            channels = [[] for i in range(chan)]\n",
    "            for i in range(0, np.int(samp_rate / chk_size * rec_time)):\n",
    "                songdata = astream.read(chk_size)\n",
    "                nums = np.fromstring(songdata, dtype=np.int16)\n",
    "                for c in range(chan):\n",
    "                    channels[c].extend(nums[c::chan])\n",
    "            # Close audio stream\n",
    "            astream.stop_stream()\n",
    "            astream.close()\n",
    "            p.terminate()\n",
    "        else:\n",
    "            sys.exit('Audio recording did not start. Start over again.')\n",
    "        # Plot time signal\n",
    "        if plot:\n",
    "            p1 = figure(plot_width=900, plot_height=500, title='Audio Signal', \n",
    "                        x_axis_label='Time (s)', y_axis_label='Amplitude (arb. units)')\n",
    "            time = np.linspace(0, len(channels[0])/framerate, len(channels[0]))\n",
    "            p1.line(time[0::100], channels[0][0::100])\n",
    "            show(p1)\n",
    "        return channels, framerate\n",
    "        \n",
    "    # Generate and plot spectrogram of audio data\n",
    "    def generate_spectrogram(self, plot, channels, framerate):\n",
    "        audiosignal = np.sum(channels, axis=0) / len(channels)  # Averaging signal over all channels\n",
    "        fs = framerate  # sampling rate\n",
    "        window = 'hamming'  # window function\n",
    "        noverlap = int(overlap_ratio * nperseg)  # number of points to overlap\n",
    "        # generate spectrogram from consecutive FFTs over the defined window\n",
    "        f, t, sgram = spectrogram(audiosignal, fs, window, nperseg, noverlap)  \n",
    "        sgram = 10 * np.log10(sgram)  # transmorm linear output to dB scale \n",
    "        sgram[sgram == -np.inf] = 0  # replace infs with zeros\n",
    "        # Plot Spectrogram\n",
    "        if plot:\n",
    "            p2 = figure(plot_width=900, plot_height=500, title='Spectrogram',\n",
    "                        x_axis_label='Time (s)', y_axis_label='Frequency (Hz)',\n",
    "                        x_range=(min(t), max(t)), y_range=(min(f), max(f)))\n",
    "            p2.image([sgram[::2, ::2]], x=min(t), y=min(f), \n",
    "                     dw=max(t), dh=max(f), palette='Spectral11')\n",
    "            show(p2)\n",
    "        return f, t, sgram\n",
    "        \n",
    "    # Find peaks in the spectrogram using image processing\n",
    "    def find_peaks(self, plot, f, t, sgram):\n",
    "        coordinates = peak_local_max(sgram, min_distance=min_peak_sep, indices=True,\n",
    "                                     threshold_abs=min_peak_amp)\n",
    "        \n",
    "        peaks = sgram[coordinates[:, 0], coordinates[:, 1]]\n",
    "        tp = t[coordinates[:, 1]]\n",
    "        fp = f[coordinates[:, 0]]\n",
    "        # Plot the peaks detected on the spectrogram\n",
    "        if plot:\n",
    "            p3 = figure(plot_width=900, plot_height=500, title='Spectrogram with Peaks',\n",
    "                        x_axis_label='Time (s)', y_axis_label='Frequency (Hz)',\n",
    "                        x_range=(min(t), max(t)), y_range=(min(f), max(f)))\n",
    "            p3.image([sgram[::2, ::2]], x=min(t), y=min(f), \n",
    "                     dw=max(t), dh=max(f), palette='Spectral11')\n",
    "            p3.scatter(tp, fp)\n",
    "            show(p3)\n",
    "        return fp, tp, peaks\n",
    "        \n",
    "    # Use the peak data from the spectrogram to generate a string with pairs of \n",
    "    # peak frequencies and the time delta between them.\n",
    "    def generate_fingerprint(self, plot, fp, tp, peaks):\n",
    "        # Create the data to be used for fingerprinting\n",
    "        # for each frequency (anchor) find the next few frequencies (targets) and calculate their time deltas\n",
    "        # the anchor-target frequency pairs and their time deltas will be used to generate the fingerprints\n",
    "        s = []  # Empty list to contain data for fingerprint\n",
    "        for i in range(len(peaks)):\n",
    "            for j in range(1, peak_connectivity):\n",
    "                if (i + j) < len(peaks):\n",
    "                    f1 = fp[i]\n",
    "                    f2 = fp[i + j]\n",
    "                    t1 = tp[i]\n",
    "                    t2 = tp[i + j]\n",
    "                    t_delta = t2 - t1\n",
    "                    if t_delta >= peak_time_delta_min and t_delta <= peak_time_delta_max:\n",
    "                        s.append(str(np.rint(f1)) + str(np.rint(f2)) + str(np.rint(t_delta))) \n",
    "        self.fingerprint = MinHash(num_perm=256)\n",
    "        for data in s:\n",
    "            self.fingerprint.update(data.encode('utf8'))\n",
    "        if plot:\n",
    "            print('{} audio-fingerprint: '.format(self.songname))\n",
    "            print(self.fingerprint.digest())\n",
    "    \n",
    "    # Save the AudioFP object to file for later use\n",
    "    def save_fingerprint(self):\n",
    "        filename = self.songname + '.pkl'\n",
    "        with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "            pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fingerprints of two songs \n",
    "\n",
    "def compare_fingerprints(s1, s2):\n",
    "    jac_sim = s1.fingerprint.jaccard(s2.fingerprint)\n",
    "    if jac_sim >= 0.9:\n",
    "        print('{} and {} are identical!'.format(s1.songname, s2.songname))\n",
    "        print('Jaccard similarity = ', jac_sim)\n",
    "    elif jac_sim >= 0.1 and jac_sim < 0.9:\n",
    "        print('{} and {} are quite similar'.format(s1.songname, s2.songname))\n",
    "        print('Jaccard similarity = ', jac_sim)\n",
    "    elif jac_sim >= 0.05 and jac_sim < 0.1:\n",
    "        print('{} and {} might have some similarity'.format(s1.songname, s2.songname))\n",
    "        print('Jaccard similarity = ', jac_sim)\n",
    "    else:\n",
    "        print('{} and {} are different'.format(s1.songname, s2.songname))\n",
    "        print('Jaccard similarity = ', jac_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create AudioFP object for first song\n",
    "song1 = AudioFP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create AudioFP object for second song\n",
    "song2 = AudioFP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the fingerprints to check their similarity\n",
    "compare_fingerprints(song1, song2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
